{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tarea con Polars**"
      ],
      "metadata": {
        "id": "7gG_jl5-NiKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Primer Ejercicio:**Desarrollo generado a partir de la explicación desarrollada por el profesor Gerardo en la clase teniendo presente las diferentes probabilidades condicionales."
      ],
      "metadata": {
        "id": "Z0ksoqMNiZKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4vQc-t-Iu0X"
      },
      "outputs": [],
      "source": [
        "#Desarrollo de los conjuntos base\n",
        "\n",
        "conjuntos = [\n",
        "    {'a','b','c'},\n",
        "    {'a','b'},\n",
        "    {'a','c','d'},\n",
        "    {'a'},\n",
        "    {'b','c'},\n",
        "    {'b','c','d'}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "#Conversión a DataFrame de Polars\n",
        "df = pl.DataFrame({\n",
        "    \"id\": range(1, len(conjuntos) + 1),  # Identificador opcional\n",
        "    \"set\": conjuntos\n",
        "})"
      ],
      "metadata": {
        "id": "QaCn_hF7L8Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "# Function to calculate conditional probability P(A | B)\n",
        "def probabilidad_condicional(df, A, B):\n",
        "    total_conjuntos = df.height  # Total of subsets\n",
        "\n",
        "    # Filter subsets containing B using `map_elements` outside the filter context\n",
        "    is_subset_B = df[\"set\"].map_elements(lambda s: B.issubset(s))\n",
        "    subconjuntos_B = df.filter(is_subset_B)\n",
        "\n",
        "    # Filter subsets containing A and B within the B subset using `apply` outside filter context\n",
        "    is_subset_A_given_B = subconjuntos_B[\"set\"].map_elements(lambda s: A.issubset(s))\n",
        "    subconjuntos_AyB = subconjuntos_B.filter(is_subset_A_given_B)\n",
        "\n",
        "    # Calculate conditional probability\n",
        "    P_B = len(subconjuntos_B) / total_conjuntos if total_conjuntos > 0 else 0\n",
        "    P_A_dado_B = len(subconjuntos_AyB) / len(subconjuntos_B) if len(subconjuntos_B) > 0 else 0\n",
        "\n",
        "    return P_A_dado_B\n",
        "\n",
        "# Example: P({'c'} | {'a'})\n",
        "A = {'c'}\n",
        "B = {'a'}\n",
        "resultado = probabilidad_condicional(df, A, B)\n",
        "\n",
        "print(f\"P({A} | {B}) =\", resultado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LE9V16wNhVo",
        "outputId": "9512fd7d-3209-4a3f-c5bc-cee06830246f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P({'c'} | {'a'}) = 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-056bbbf88f1f>:8: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
            "  is_subset_B = df[\"set\"].map_elements(lambda s: B.issubset(s))\n",
            "<ipython-input-14-056bbbf88f1f>:12: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
            "  is_subset_A_given_B = subconjuntos_B[\"set\"].map_elements(lambda s: A.issubset(s))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algoritmo Apriori:** Para el primer ejercicio se tiene:"
      ],
      "metadata": {
        "id": "vrFe8-VjWq8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el desarrollo del algoritmo tuve presente lo siguiente:\n",
        "\n",
        "*   **Conteo Inicial:** Me permitió contar la frecuencia de cada ítem individual.\n",
        "*   **Generación de Candidatos:** Usa combinación para crear todos los posibles itemsets de tamaño k+1, teniendo presente que mi cojunto de ítems en este caso serían los conjuntos que el profesor colocó en el tablero.\n",
        "*   **Podado:** Elimina candidatos donde algún subconjunto de tamaño k no sea frecuente.\n",
        "*   **Cálculo de Soporte:** Cuenta cuántas transacciones contienen cada candidato.\n",
        "*   **Filtrado:** Conserva solo los conjuntos que cumplen con el soporte mínimo\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1GMYjAtn4X9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "#Desarrollo de diferentes conjuntos de base de datos\n",
        "conjuntos = [\n",
        "    {'a','b','c'},\n",
        "    {'a','b'},\n",
        "    {'a','c','d'},\n",
        "    {'a'},\n",
        "    {'b','c'},\n",
        "    {'b','c','d'}\n",
        "]\n",
        "\n",
        "def apriori(datos, soporte_min):\n",
        "    # Paso 1: Encontrar itemsets frecuentes de tamaño 1\n",
        "    conteo = defaultdict(int)\n",
        "    for transaccion in datos:\n",
        "        for item in transaccion:\n",
        "            conteo[frozenset([item])] += 1\n",
        "\n",
        "    frecuentes = [\n",
        "        [item for item, count in conteo.items() if count >= soporte_min],\n",
        "    ]\n",
        "\n",
        "    k = 1\n",
        "    while True:\n",
        "        # Paso 2: Generar candidatos de tamaño k+1\n",
        "        items = list({item for itemset in frecuentes[k-1] for item in itemset})\n",
        "        candidatos = [frozenset(comb) for comb in combinations(items, k+1)]\n",
        "        '''La función frozenset transforma cualquier objeto iterable en un\n",
        "        objeto inmutable'''\n",
        "\n",
        "        # Paso 3: Podar candidatos\n",
        "        candidatos_podados = []\n",
        "        for candidato in candidatos:\n",
        "            subconjuntos = [frozenset(s) for s in combinations(candidato, k)]\n",
        "            if all(sub in frecuentes[k-1] for sub in subconjuntos):\n",
        "                candidatos_podados.append(candidato)\n",
        "\n",
        "        # Paso 4: Calcular soporte\n",
        "        conteo_candidatos = defaultdict(int)\n",
        "        for transaccion in datos:\n",
        "            for candidato in candidatos_podados:\n",
        "                if candidato.issubset(transaccion):\n",
        "                    conteo_candidatos[candidato] += 1\n",
        "\n",
        "        # Filtrar por soporte mínimo\n",
        "        nuevos_frecuentes = [item for item, count in conteo_candidatos.items() if count >= soporte_min]\n",
        "\n",
        "        if not nuevos_frecuentes:\n",
        "            break\n",
        "\n",
        "        frecuentes.append(nuevos_frecuentes)\n",
        "        k += 1\n",
        "\n",
        "    return frecuentes\n",
        "\n",
        "# Ejecutar con soporte mínimo = 2 (33% de los conjuntos)\n",
        "resultado = apriori(conjuntos, 2)\n",
        "\n",
        "# Mostrar resultados\n",
        "for i, nivel in enumerate(resultado):\n",
        "    print(f\"Itemsets frecuentes de tamaño {i+1}:\")\n",
        "    for itemset in nivel:\n",
        "        print(f\"   {set(itemset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpUFfiIjyGSi",
        "outputId": "b1b42096-7cc0-4c11-e2a2-fed8fb4b7d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemsets frecuentes de tamaño 1:\n",
            "   {'a'}\n",
            "   {'b'}\n",
            "   {'c'}\n",
            "   {'d'}\n",
            "Itemsets frecuentes de tamaño 2:\n",
            "   {'a', 'b'}\n",
            "   {'a', 'c'}\n",
            "   {'b', 'c'}\n",
            "   {'c', 'd'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte de código con probabilidad de confianza:** Teniendo presente que en la clase se expuso todo en términos de probabilidades, se comparte una visión donde se plantea el algoritmo en torno a la visualización de probabilidades."
      ],
      "metadata": {
        "id": "1DNd-Bme6Hla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# Configuración pensando en términos de probabilidades.\n",
        "conjuntos = [\n",
        "    {'a','b','c'},\n",
        "    {'a','b'},\n",
        "    {'a','c','d'},\n",
        "    {'a'},\n",
        "    {'b','c'},\n",
        "    {'b','c','d'}\n",
        "]\n",
        "min_support = 2\n",
        "min_confidence = 0.5  # 50%\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1. Algoritmo Apriori (Modificado para guardar soportes y tener presente probabilidades)\n",
        "# ------------------------------------------\n",
        "def apriori_with_support(datos, soporte_min):\n",
        "    support_dict = {}\n",
        "    # Paso 1: Itemsets de tamaño 1\n",
        "    conteo = defaultdict(int)\n",
        "    for trans in datos:\n",
        "        for item in trans:\n",
        "            key = frozenset([item])\n",
        "            conteo[key] += 1\n",
        "    support_dict.update(conteo)\n",
        "    frecuentes = [[k for k, v in conteo.items() if v >= soporte_min]]\n",
        "\n",
        "    k = 1\n",
        "    while True:\n",
        "        # Generar candidatos\n",
        "        items = list({i for s in frecuentes[-1] for i in s})\n",
        "        candidatos = [frozenset(c) for c in combinations(items, k+1)]\n",
        "\n",
        "        # Podar y calcular soporte\n",
        "        conteo_candidatos = defaultdict(int)\n",
        "        for trans in datos:\n",
        "            for cand in candidatos:\n",
        "                if cand.issubset(trans):\n",
        "                    conteo_candidatos[cand] += 1\n",
        "        support_dict.update(conteo_candidatos)\n",
        "\n",
        "        nuevos_frec = [k for k, v in conteo_candidatos.items() if v >= soporte_min]\n",
        "        if not nuevos_frec:\n",
        "            break\n",
        "        frecuentes.append(nuevos_frec)\n",
        "        k += 1\n",
        "\n",
        "    return support_dict\n",
        "\n",
        "# Ejecutar Apriori\n",
        "apriori_support = apriori_with_support(conjuntos, min_support)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3. Función común para generar reglas\n",
        "# ------------------------------------------\n",
        "def generar_reglas(support_dict, min_conf):\n",
        "    reglas = []\n",
        "    itemsets = [k for k in support_dict.keys() if len(k) > 1]\n",
        "\n",
        "    for itemset in itemsets:\n",
        "        items = list(itemset)\n",
        "        # Generar todos los posibles antecedentes\n",
        "        for i in range(1, len(items)):\n",
        "            for antecedente in combinations(items, i):\n",
        "                antecedente = frozenset(antecedente)\n",
        "                consecuente = itemset - antecedente\n",
        "                soporte_total = support_dict[itemset]\n",
        "                soporte_antecedente = support_dict.get(antecedente, 0)\n",
        "\n",
        "                if soporte_antecedente == 0:\n",
        "                    continue\n",
        "\n",
        "                confianza = soporte_total / soporte_antecedente\n",
        "                if confianza >= min_conf:\n",
        "                    reglas.append({\n",
        "                        'regla': f\"{set(antecedente)} => {set(consecuente)}\",\n",
        "                        'soporte': soporte_total,\n",
        "                        'confianza': round(confianza, 2)\n",
        "                    })\n",
        "    return reglas\n",
        "\n",
        "# Generar reglas para ambos algoritmos\n",
        "reglas_apriori = generar_reglas(apriori_support, min_confidence)\n",
        "\n",
        "# 4. Resultados\n",
        "# ------------------------------------------\n",
        "print(\"Reglas de Apriori (Confianza ≥ 50%):\")\n",
        "for regla in reglas_apriori:\n",
        "    print(f\"{regla['regla']} | Soporte: {regla['soporte']}, Confianza: {regla['confianza']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek4YAsFN6G9C",
        "outputId": "00370cef-00d9-4cf4-e7d4-db4f354128e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reglas de Apriori (Confianza ≥ 50%):\n",
            "{'a'} => {'b'} | Soporte: 2, Confianza: 0.5\n",
            "{'b'} => {'a'} | Soporte: 2, Confianza: 0.5\n",
            "{'a'} => {'c'} | Soporte: 2, Confianza: 0.5\n",
            "{'c'} => {'a'} | Soporte: 2, Confianza: 0.5\n",
            "{'b'} => {'c'} | Soporte: 3, Confianza: 0.75\n",
            "{'c'} => {'b'} | Soporte: 3, Confianza: 0.75\n",
            "{'d'} => {'a'} | Soporte: 1, Confianza: 0.5\n",
            "{'c'} => {'d'} | Soporte: 2, Confianza: 0.5\n",
            "{'d'} => {'c'} | Soporte: 2, Confianza: 1.0\n",
            "{'d'} => {'b'} | Soporte: 1, Confianza: 0.5\n",
            "{'a', 'b'} => {'c'} | Soporte: 1, Confianza: 0.5\n",
            "{'a', 'c'} => {'b'} | Soporte: 1, Confianza: 0.5\n",
            "{'d'} => {'a', 'c'} | Soporte: 1, Confianza: 0.5\n",
            "{'a', 'c'} => {'d'} | Soporte: 1, Confianza: 0.5\n",
            "{'a', 'd'} => {'c'} | Soporte: 1, Confianza: 1.0\n",
            "{'c', 'd'} => {'a'} | Soporte: 1, Confianza: 0.5\n",
            "{'d'} => {'b', 'c'} | Soporte: 1, Confianza: 0.5\n",
            "{'b', 'd'} => {'c'} | Soporte: 1, Confianza: 1.0\n",
            "{'c', 'd'} => {'b'} | Soporte: 1, Confianza: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algoritmo FP-Growth:**Para el primer ejercicio se tiene lo siguiente:"
      ],
      "metadata": {
        "id": "MxU658fh1uU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "conjuntos = [\n",
        "    {'a','b','c'},\n",
        "    {'a','b'},\n",
        "    {'a','c','d'},\n",
        "    {'a'},\n",
        "    {'b','c'},\n",
        "    {'b','c','d'}\n",
        "]\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, name, parent):\n",
        "        self.name = name       # Nombre del ítem\n",
        "        self.parent = parent   # Nodo padre\n",
        "        self.children = {}     # Diccionario de hijos {nombre: nodo}\n",
        "        self.count = 0         # Contador de soporte\n",
        "        self.link = None       # Enlace a nodos del mismo ítem\n",
        "\n",
        "def build_fp_tree(conjuntos, min_support):\n",
        "    # Paso 1: Calcular frecuencia de ítems\n",
        "    item_counts = defaultdict(int)\n",
        "    for trans in conjuntos:\n",
        "        for item in trans:\n",
        "            item_counts[item] += 1\n",
        "\n",
        "    # Filtrar ítems frecuentes y ordenar\n",
        "    freq_items = {item: count for item, count in item_counts.items() if count >= min_support}\n",
        "    if not freq_items:\n",
        "        return None, None\n",
        "    sorted_items = sorted(freq_items.items(), key=lambda x: (-x[1], x[0]))\n",
        "    order = [item[0] for item in sorted_items]\n",
        "\n",
        "    # Paso 2: Construir FP-Tree y tabla de cabecera\n",
        "    root = TreeNode('root', None)\n",
        "    header_table = {item: [count, None] for item, count in sorted_items}\n",
        "\n",
        "    for trans in conjuntos:\n",
        "        # Filtrar y ordenar transacción\n",
        "        filtered = [item for item in trans if item in freq_items]\n",
        "        filtered.sort(key=lambda x: (order.index(x), x))\n",
        "        if not filtered:\n",
        "            continue\n",
        "\n",
        "        # Insertar en el árbol\n",
        "        current_node = root\n",
        "        for item in filtered:\n",
        "            if item in current_node.children:\n",
        "                current_node = current_node.children[item]\n",
        "                current_node.count += 1\n",
        "            else:\n",
        "                new_node = TreeNode(item, current_node)\n",
        "                new_node.count = 1\n",
        "                current_node.children[item] = new_node\n",
        "\n",
        "                # Enlazar a la tabla de cabecera\n",
        "                if header_table[item][1] is None:\n",
        "                    header_table[item][1] = new_node\n",
        "                else:\n",
        "                    node = header_table[item][1]\n",
        "                    while node.link is not None:\n",
        "                        node = node.link\n",
        "                    node.link = new_node\n",
        "                current_node = new_node\n",
        "\n",
        "    return root, header_table\n",
        "\n",
        "def mine_fp_tree(header_table, min_support, prefix, freq_itemsets):\n",
        "    items = list(header_table.items())\n",
        "    # Procesar desde el ítem menos frecuente\n",
        "    for item, (count, node) in sorted(items, key=lambda x: (x[1][0], x[0])):\n",
        "        new_prefix = prefix.copy()\n",
        "        new_prefix.add(item)\n",
        "        freq_itemsets.append((new_prefix, count))  # Añadir itemset actual\n",
        "\n",
        "        # Paso 3: Encontrar patrones condicionales\n",
        "        conditional_paths = []\n",
        "        current_node = node\n",
        "        while current_node is not None:\n",
        "            path = []\n",
        "            parent = current_node.parent\n",
        "            while parent.name != 'root':\n",
        "                path.append(parent.name)\n",
        "                parent = parent.parent\n",
        "            path.reverse()\n",
        "            if path:\n",
        "                conditional_paths.append((path, current_node.count))\n",
        "            current_node = current_node.link\n",
        "\n",
        "        # Construir transacciones condicionales\n",
        "        conditional_trans = []\n",
        "        for path, cnt in conditional_paths:\n",
        "            conditional_trans.extend([path] * cnt)\n",
        "\n",
        "        # Construir sub-árbol condicional\n",
        "        sub_root, sub_header = build_fp_tree(conditional_trans, min_support)\n",
        "        if sub_header is not None:\n",
        "            mine_fp_tree(sub_header, min_support, new_prefix, freq_itemsets)\n",
        "\n",
        "def fp_growth(transacciones, min_support):\n",
        "    freq_itemsets = []\n",
        "    root, header_table = build_fp_tree(transacciones, min_support)\n",
        "    if header_table:\n",
        "        mine_fp_tree(header_table, min_support, set(), freq_itemsets)\n",
        "    # Filtrar y formatear resultados\n",
        "    result = defaultdict(int)\n",
        "    for itemset, count in freq_itemsets:\n",
        "        key = frozenset(itemset)\n",
        "        result[key] = max(result[key], count)\n",
        "    return sorted([(set(k), v) for k, v in result.items()], key=lambda x: (len(x[0]), x[0]))\n",
        "\n",
        "# Ejecutar con soporte mínimo = 2\n",
        "resultado = fp_growth(conjuntos, 2)\n",
        "\n",
        "# Mostrar resultados Básicos de la configuración de los diferentes ítems\n",
        "print(\"Itemsets frecuentes con FP-Growth:\")\n",
        "for itemset, soporte in resultado:\n",
        "    print(f\"{set(itemset)}: Soporte = {soporte}\")\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3. Función común para generar reglas\n",
        "# ------------------------------------------\n",
        "def generar_reglas(support_dict, min_conf):\n",
        "    reglas = []\n",
        "    itemsets = [k for k in support_dict.keys() if len(k) > 1]\n",
        "\n",
        "    for itemset in itemsets:\n",
        "        items = list(itemset)\n",
        "        # Generar todos los posibles antecedentes\n",
        "        for i in range(1, len(items)):\n",
        "            for antecedente in combinations(items, i):\n",
        "                antecedente = frozenset(antecedente)\n",
        "                consecuente = itemset - antecedente\n",
        "                soporte_total = support_dict[itemset]\n",
        "                soporte_antecedente = support_dict.get(antecedente, 0)\n",
        "\n",
        "                if soporte_antecedente == 0:\n",
        "                    continue\n",
        "\n",
        "                confianza = soporte_total / soporte_antecedente\n",
        "                if confianza >= min_conf:\n",
        "                    reglas.append({\n",
        "                        'regla': f\"{set(antecedente)} => {set(consecuente)}\",\n",
        "                        'soporte': soporte_total,\n",
        "                        'confianza': round(confianza, 2)\n",
        "                    })\n",
        "    return reglas\n",
        "\n",
        "# Generar reglas para ambos algoritmos\n",
        "reglas_fp_growth = generar_reglas(fp_growth_support, min_confidence)\n",
        "\n",
        "print(\"\\nReglas de FP-Growth (Confianza ≥ 50%):\")\n",
        "for regla in reglas_fp_growth:\n",
        "    print(f\"{regla['regla']} | Soporte: {regla['soporte']}, Confianza: {regla['confianza']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAp_sa__0yPd",
        "outputId": "c404d48e-8616-4e85-a6df-5d9356ee4500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemsets frecuentes con FP-Growth:\n",
            "{'d'}: Soporte = 2\n",
            "{'a'}: Soporte = 4\n",
            "{'b'}: Soporte = 4\n",
            "{'c'}: Soporte = 4\n",
            "{'c', 'd'}: Soporte = 2\n",
            "{'a', 'b'}: Soporte = 2\n",
            "{'a', 'c'}: Soporte = 2\n",
            "{'c', 'b'}: Soporte = 3\n",
            "\n",
            "Reglas de FP-Growth (Confianza ≥ 50%):\n",
            "{'c'} => {'d'} | Soporte: 2, Confianza: 0.5\n",
            "{'d'} => {'c'} | Soporte: 2, Confianza: 1.0\n",
            "{'a'} => {'b'} | Soporte: 2, Confianza: 0.5\n",
            "{'b'} => {'a'} | Soporte: 2, Confianza: 0.5\n",
            "{'a'} => {'c'} | Soporte: 2, Confianza: 0.5\n",
            "{'c'} => {'a'} | Soporte: 2, Confianza: 0.5\n",
            "{'c'} => {'b'} | Soporte: 3, Confianza: 0.75\n",
            "{'b'} => {'c'} | Soporte: 3, Confianza: 0.75\n"
          ]
        }
      ]
    }
  ]
}